import json
import numpy as np
import telebot
from telebot import types
from transformers import BertTokenizer, BertForSequenceClassification

with open('/home/vlamykin/nlp-config.json', 'r') as f:
    json_config = json.load(f)

TOKEN = json_config['tg_bot_token']

bot = telebot.TeleBot(TOKEN, num_threads=4)

@bot.message_handler(commands=['start'])
def start_message(message):
    bot.send_message(message.chat.id,"""–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å! ü§ñ –Ø - –≤–∞—à –Ω–∞–¥–µ–∂–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á WiC: The Word-in-Context Dataset. –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–¥–µ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç-—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤.

–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –≤–∞–º –∫–∞–∂–µ—Ç—Å—è –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω—ã–º, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –µ–≥–æ –º–Ω–µ –≤–º–µ—Å—Ç–µ —Å –¥–≤—É–º—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏(–≤ —Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏, –∏–º–µ–π—Ç–µ –≤–≤–∏–¥—É —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ 150 —Å–∏–º–≤–æ–ª–æ–≤ –∏ –±—É–¥—É—Ç –æ–±—Ä–µ–∑–∞—Ç—å—Å—è), –∏ —è –≤–∞–º —Å–∫–∞–∂—É, –≤–µ—Ä–Ω–æ –ª–∏ —É–≥–∞–¥–∞–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ù–∞–ø—Ä–∏–º–µ—Ä:

```yaml
–°–ª–æ–≤–æ: –¥–æ—Ä–æ–∂–∫–∞
–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1: –ë—É—Ä—ã–µ –∫–æ–≤—Ä–æ–≤—ã–µ –¥–æ—Ä–æ–∂–∫–∏ –∑–∞–≥–ª—É—à–∞–ª–∏ —à–∞–≥–∏
–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2: –ü—Ä–∏—è—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –≤—ã–ø–∏—Ç—å –Ω–∞ –¥–æ—Ä–æ–∂–∫—É –≤ –º–µ—Å—Ç–Ω–æ–º –±–∞—Ä–µ
```

–ü–æ—Å–ª–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—é –≤–∞–º –æ—Ç–≤–µ—Ç –≤ –≤–∏–¥–µ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (true/false) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –≤–∞—à–µ–≥–æ –≤—ã–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º! üöÄ""",
                     parse_mode="Markdown")

tokenizer = BertTokenizer.from_pretrained("./model_save/tokenizer_epoch_3", do_lower_case=True)
model = BertForSequenceClassification.from_pretrained(
    "./model_save/model_epoch_3",
    num_labels=2,                       
    output_attentions=False,
    output_hidden_states=False,
)
device="cpu"
model.to(device)

def encode(sentence):  
    encoded_dict = tokenizer.encode_plus(
                    sentence,
                    add_special_tokens = True,
                    max_length = 168,
                    pad_to_max_length = True,
                    return_attention_mask = True,
                    return_tensors = 'pt',
            )                

    return encoded_dict['input_ids'], encoded_dict['attention_mask']

def inference(input,attention_mask):
    b_input_ids, b_input_mask = input.to(device),attention_mask.to(device)
    outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask)
    logit = outputs[0].cpu().detach().numpy()[0]
    result = np.argmax(logit)
    return result==1

def get_sentence(text):
    lines = text.split('\n')
    sentence_1 = ""
    sentence_2 = ""
    word = ""
    for line in lines:
        if line.startswith("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1: "):
            sentence_1 = line[len("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1:"):].strip()[:150]
        elif line.startswith("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2:"):
            sentence_2 = line[len("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2:"):].strip()[:150]
        elif line.startswith("–°–ª–æ–≤–æ: "):
            word = line[len("–°–ª–æ–≤–æ: "):].strip()

    return f"{sentence_1}. {sentence_2}. {word}"


@bot.message_handler()
def log_message(message):
    print(message, end='\n')
    sentence=get_sentence(message.text)
    input,attention_mask=encode(sentence)
    result=inference(input,attention_mask)
    bot.send_message(message.chat.id, f'–Ø –¥—É–º–∞—é {result}\n---\n–î–∞–≤–∞–π –µ—â–µ =)\n---')

bot.infinity_polling()